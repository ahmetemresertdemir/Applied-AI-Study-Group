{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Computer Vision\n",
    "\n",
    "## Notebook 4: Semantic Segmentation with a U-Net Convolutional Neural Network using PyTorch\n",
    "\n",
    "Welcome to the fourth notebook of this week's Applied AI Study Group! We will study semantic segmentation problem with MSRC-v2 image dataset provided by Microsoft. The aim of our task will be to make object segmentation in the given images.\n",
    "\n",
    "### 1. Semantic Segmentation\n",
    "\n",
    "Semantic Segmentation aims to label each pixel (aka classify pixel-wise) of a given image. We treat different objects of the same class as they are same object. On contrast, instance segmentation treats each objects of the same class as they are different objects, hence, label them differently such as object 1, object 2, etc. In this notebook, we will tackle the problem of semantic segmentation. The pixel-wise operations can be applied via segmentation on images, for example, portrait mode in images requires to differentiate between foreground and background of an image. We blur out the pixels which are classified as background. \n",
    "\n",
    "So, how do we build our model for this case? We know that the capabilities of convolution filters are proven in terms of their capabilities in processing structured data such as images. However, they reduce the size of their input vector depends on their kernel size. We need a model that outputs the same size of input vector since we want to retrieve the same image we give into the model. Luckily for us, we have U-Net Architecture for these kind of tasks. We will study U-Nets in the following section.\n",
    "\n",
    "### 2. U-Net Convolutional Neural Network\n",
    "\n",
    "U-Net\n",
    "\n",
    "### 3. Imports and Checks\n",
    "\n",
    "You should have installed Numpy and Matplotlib using `pip` and, PyTorch using [Week 0 - Notebook 2](https://github.com/inzva/Applied-AI-Study-Group/blob/add-frameworks-week/Applied%20AI%20Study%20Group%20%236%20-%20January%202022/Week%200/2-mnist_classification_convnet_pytorch.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from datasets.segmentation_dataset import SegmentationData, label_img_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join('./datasets','segmentation')\n",
    "\n",
    "train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n",
    "test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SegmentationNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=23, hparams=None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        mobile_network = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        layers = list(mobile_network.children())[:-1]  # 1x1280x8x8\n",
    "        layers.append(nn.Conv2d(1280, 120, 1, 1))  # 1x160x8x8\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=4))  # 1x160x32x32\n",
    "        layers.append(nn.ConvTranspose2d(120, 80, 3, 2))  # 1x120x64x64\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(80, 60, 9, dilation=2))  # 1x80x80x80\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(60, 40, 9, dilation=2))  # 1x60x96x96\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(40, 40, 11, dilation=2))  # 1x40x116x116\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=2))  # 1x40x232x232\n",
    "        layers.append(nn.ConvTranspose2d(40, 23, 7))  # 1x23x240x240\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.001,\n",
    "    \"batch_size\" : 4,\n",
    "    \"num_epochs\" : 4\n",
    "}  \n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "print(train_loader)\n",
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inputs, targets) in train_data[0:4]:\n",
    "    inputs, targets = inputs, targets\n",
    "    outputs = model(inputs.unsqueeze(0).to(device))\n",
    "    losses = criterion(outputs, targets.unsqueeze(0).to(device))\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training starts!')\n",
    "for epoch in range(hparams[\"num_epochs\"]):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: %d Loss: %.3f\" % (epoch + 1, epoch_loss / 276))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
