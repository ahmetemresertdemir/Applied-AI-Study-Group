{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Computer Vision\n",
    "\n",
    "## Notebook 5: Instance Segmentation with a Pre-Trained Model using Torchvision\n",
    "\n",
    "Welcome to the fifth notebook of this week's Applied AI Study Group! We will study instance segmentation problem in this notebook. The aim of our task will be to identify each object in the given images and differentiate the instances even the ones that belong to the same category.\n",
    "\n",
    "### 1. Instance Segmentation\n",
    "\n",
    "Instance segmentation treats each objects of the same class as they are different objects, hence, label them differently such as object 1, object 2, etc. Its difference from [Semantic Segmentation](https://github.com/inzva/Applied-AI-Study-Group/blob/add-cv-week1/Applied%20AI%20Study%20Group%20%236%20-%20January%202022/Week%201/4-segmentation_uNet_PyTorch_final.ipynb) is its aim towards differentiating every single object in the given images. It is a more challenging problem since we want to identify each object separately comparing to having one entity corresponding multiple object.\n",
    "\n",
    "### 2. `Some model...` Convolutional Neural Network\n",
    "\n",
    "TODO: will add model explanation.\n",
    "\n",
    "### 3. Torchvision\n",
    "\n",
    "Torchvision package is built on PyTorch. It aims to provide popular datasets, model (including pretrained models) architectures, and image transformation techniques applied in the field of computer vision.\n",
    "\n",
    "To install Torchvision:\n",
    "\n",
    "    !pip install torchvision\n",
    "\n",
    "### 4. Imports and Checks\n",
    "\n",
    "You should have installed Numpy and Matplotlib using `pip` and, PyTorch using [Week 0 - Notebook 2](https://github.com/inzva/Applied-AI-Study-Group/blob/add-frameworks-week/Applied%20AI%20Study%20Group%20%236%20-%20January%202022/Week%200/2-mnist_classification_convnet_pytorch.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells will import required libraries and packages to run this notebook and will download the model we will use. \n",
    "\n",
    "TODO: will add model name and 1 sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: will update to instance segmentation\n",
    "# In this example we use a pretrained model to perform segmentation\n",
    "# here is the original code: https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/\n",
    "from torchvision import models\n",
    "\n",
    "fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the label colors for each type of object.\n",
    "# lets say, for aeroplanes, we color the pixels red.\n",
    "# for bicycles, we paint the pixels green.\n",
    "def decode_segmap(image, nc=21):\n",
    "    label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "  \n",
    "    for l in range(0, nc):\n",
    "        idx = image == l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "    \n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(net, path):\n",
    "    img = Image.open(path)\n",
    "    plt.imshow(img); plt.axis('off'); plt.show()\n",
    "    # pytorch transforms perform operations to data step by step. it is a way to do preprocessing.\n",
    "    trf = T.Compose([T.Resize(256), \n",
    "                   T.CenterCrop(224), \n",
    "                   T.ToTensor(), \n",
    "                   T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                               std = [0.229, 0.224, 0.225])])\n",
    "  \n",
    "    # cannot input one image. instead we input a dataset composed of one image. this operation is for that, we add another dimension.\n",
    "    inp = trf(img).unsqueeze(0)\n",
    "    #make forward prop, take the output tensor\n",
    "    out = net(inp)['out']\n",
    "    # take the class prediction with the maximum probability\n",
    "    om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "    #pass the class to the coloring function\n",
    "    rgb = decode_segmap(om)\n",
    "    #draw the colored image\n",
    "    plt.imshow(rgb); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://images.pexels.com/photos/2385051/pexels-photo-2385051.jpeg\n",
    "#!ls\n",
    "# point out the resize!\n",
    "segment(fcn, './datasets/segmentation.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
